# Mantis Framework - Artifact Reproduction Guide

This repository contains the source code and scripts to reproduce all tables and figures from the paper "Mantis: An Open-Source Framework for SysMixer-Based Telemetry Forecasting and Anomaly Detection in High-Performance Computing".

## Quick Setup

```bash
# Step 1: Setup environment
./build_env/autosetup.sh

# Step 2: Activate environment
conda activate fmhpc
```

## Source Code to Figure/Table Mapping


| Paper Element | Source File | Command |
|---------------|-------------|---------|
| Table 2 (Overview RMSE) | `evaluation/Regression_score.py` | `python evaluation/Regression_score.py` |
| Table 3 (Model Profile) | `evaluation/Profile_model.py` | `python evaluation/Profile_model.py` |
| Table 4 (Computational Cost) | `evaluation/Timing_graph_cost.py` | `python evaluation/Timing_graph_cost.py` |
| Table 5 (Graph Sensitivity) | `evaluation/Analye_graph_sensitivity.py` | `python evaluation/Analye_graph_sensitivity.py` |
| Table 6,7 (Synthetic Anomalies) | `evaluation/Eval_synthetic_anomalies.py` | `python evaluation/Eval_synthetic_anomalies.py` |
| Table 8 (Sandia Anomalies) | `evaluation/Eval_sandia_anomalies.py` | `python evaluation/Eval_sandia_anomalies.py` |
| Table 9 (OLCF Real-world Anomalies) | `evaluation/Eval_olcf_realworld_anomalies.py` | `python evaluation/Eval_olcf_realworld_anomalies.py` |
| Figure 5,6 (Graph Visualization) | `evaluation/Plot_graph.py` | `python evaluation/Plot_graph.py` |
| Figure 7 (Time Series Comparison) | `evaluation/Compare_line.py` | `python evaluation/Compare_line.py` |
| Figure 8 (Graph Evolution) | `evaluation/Plot_gns.py` | `python evaluation/Plot_gns.py` |
| Figure 9 (Anomaly Time Window) | `evaluation/Plot_anomaly_time_win.py` | `python evaluation/Plot_anomaly_time_win.py` |

## Complete Reproduction Workflow

### Step 1: Environment Setup (10 minutes)
```bash
./build_env/autosetup.sh
conda activate fmhpc
```
### Step 2: Reproduce Tables and Figures (30 minutes)
```bash
# Tables
python evaluation/Regression_score.py    # Table 2
python evaluation/Profile_model.py       # Table 3
python evaluation/Timing_graph_cost.py           # Table 4
python evaluation/Analye_graph_sensitivity.py    # Table 5
python evaluation/Eval_synthetic_anomalies.py    # Table 6,7
python evaluation/Eval_sandia_anomalies.py       # Table 8
python evaluation/Eval_olcf_realworld_anomalies.py # Table 9

# Figures
python evaluation/Plot_graph.py          # Figure 5,6
python evaluation/Compare_line.py        # Figure 7
python evaluation/Plot_gns.py           # Figure 8
python evaluation/Plot_anomaly_time_win.py # Figure 9
```

## Output Files

### Tables
- Results are displayed in the terminal
- Table 4 values may vary based on hardware platform

### Figures
- All figures saved as PNG files in root directory
- Expected files: `fig5_a.png`, `fig5_b.png`, `fig5_c.png`, `fig6_a.png`, `fig6_b.png`, `fig6_c.png`, `fig7_a.png`, `fig7_b.png`, `fig7_c.png`, `fig7_d.png`, `fig8.png`, `fig9_a.png`, `fig9_b.png`, `fig9_c.png`

## Troubleshooting

### Environment Issues
If automatic setup fails, refer to `build_env/README.md` for manual setup instructions.

## System Requirements

- **Hardware**: NVIDIA GPU recommended (tested on A100 80GB)
- **OS**: Linux (Ubuntu 22.04.3 LTS)
- **Software**: CUDA 12.3, Python 3.8.16, Conda 24.1.2

## Datasets

Three datasets are included:
- JLab telemetry (Jefferson Lab compute nodes)
- OLCF telemetry (Summit supercomputer)
- Sandia HPAS telemetry (with injected memory leaks)

